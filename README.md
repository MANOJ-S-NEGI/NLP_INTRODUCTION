# NLP_INTRODUCTION

<table>
  <tr>
    <th>SimpleRNN</th>
    <th>LSTM</th>
    <th>GRU</th>
  </tr>
  <tr>
    <td>
    A Simple Recurrent Neural Network (SimpleRNN) is a type of recurrent neural network (RNN) that processes sequences of data by maintaining a hidden state. It's a type of neural network layer that can be used for tasks 
      involving sequential or time-series data.<br><br>

  1. Recurrent Neurons:
     * SimpleRNNs have recurrent neurons, which allow them to maintain a form of memory about previous information in a sequence.

  2. Hidden State:
     * At each time step, a SimpleRNN takes both the current input and the previous hidden state as input. The hidden state acts as a form of memory that is updated and passed along through time.
  
  3. Mathematical Operation:
     * The output at each time step is computed as a function of the current input and the previous hidden state, similar to the operations in a feedforward neural network.

  4. Vanishing Gradient Problem:
     * SimpleRNNs can suffer from the vanishing gradient problem, which can make it difficult to learn long-term dependencies in sequences.
  
  5. Limited Context:
     * Due to their structure, SimpleRNNs have difficulty capturing long-term dependencies in sequences. As a result, they are often limited to tasks where the context window is relatively small.
  
  
  6. Drawbacks:
      * Due to the vanishing gradient problem, SimpleRNNs are often not suitable for tasks that require capturing long-term dependencies. More advanced recurrent units like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) have been developed to address this issue.
  
  
  
  
  <td>Maria Anders</td>
    <td>Germany</td>
  </tr>




  
  <tr>
    <td>Centro comercial Moctezuma</td>
    <td>Francisco Chang</td>
    <td>Mexico</td>
  </tr>
</table>
